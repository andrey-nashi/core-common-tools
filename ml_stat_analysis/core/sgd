import numpy as np
import matplotlib.pyplot as plt

# Generate synthetic data for logistic regression in 1D space
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = (4 + 3 * X.squeeze() + np.random.randn(100) > 6).astype(int)

# Add a bias term to X
X_b = np.c_[np.ones((100, 1)), X]

# Initialize weights randomly
theta = np.random.randn(2, 1)

# Number of iterations and learning rate
n_iterations = 1000
learning_rate = 0.01

# Sigmoid function
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# Negative log likelihood loss
def negative_log_likelihood(y_true, y_prob):
    epsilon = 1e-15
    loss = - (y_true * np.log(y_prob + epsilon) + (1 - y_true) * np.log(1 - y_prob + epsilon))
    return np.mean(loss)

# Gradient of negative log likelihood loss
def gradient(y_true, y_prob, X):
    return np.dot(X.T, (y_prob - y_true)) / len(y_true)

# Stochastic Gradient Descent for Logistic Regression in 1D space
for iteration in range(n_iterations):
    for i in range(len(X)):
        xi = X_b[i:i+1]
        yi = y[i:i+1]

        logits = sigmoid(np.dot(xi, theta))
        loss = negative_log_likelihood(yi, logits)

        # Manually compute gradients using NumPy
        grad = gradient(yi, logits, xi)

        # Update weights manually
        theta = theta - learning_rate * grad.reshape(-1, 1)

# Print the final weights
print("Final Weights (Manually Updated):", theta)

# Plot the data and decision boundary
plt.scatter(X, y)
plt.xlabel('Feature')
plt.ylabel('Target')
plt.title('Logistic Regression with SGD - Negative Log-Likelihood Loss - NumPy Implementation')

# Plot the decision boundary
x_decision_boundary = np.linspace(0, 2, 100).reshape(-1, 1)
y_decision_boundary = (theta[0] + theta[1] * x_decision_boundary)
plt.plot(x_decision_boundary, sigmoid(y_decision_boundary), color='red', label='Decision Boundary')

plt.legend()
plt.show()